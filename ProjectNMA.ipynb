{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "wO5IoepHDszf",
        "sW_LWJi2dmY8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load HCP data and connectivity analysis."
      ],
      "metadata": {
        "id": "6qqwCOj-WOOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial PCA"
      ],
      "metadata": {
        "id": "N9f_8zoT_6XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "bUJxwZOdDukd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_cov_matrix(X):\n",
        "  \"\"\"\n",
        "  Returns the sample covariance matrix of data X\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)   : Covariance matrix\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: calculate the covariance matrix\n",
        "  # Fill out function and remove\n",
        "  #raise NotImplementedError(\"Student exercise: calculate the covariance matrix!\")\n",
        "  #################################################\n",
        "\n",
        "  # Subtract the mean of X\n",
        "  X = X - np.mean(X, 0)\n",
        "\n",
        "  # Calculate the covariance matrix (hint: use np.matmul)\n",
        "  cov_matrix = 1 / X.shape[0] * np.matmul(X.T, X)\n",
        "\n",
        "  return cov_matrix"
      ],
      "metadata": {
        "id": "IdojIh1dDT-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_evals_descending(evals, evectors):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
        "  eigenvectors to be in first two quadrants (if 2D).\n",
        "\n",
        "  Args:\n",
        "    evals (numpy array of floats)    : Vector of eigenvalues\n",
        "    evectors (numpy array of floats) : Corresponding matrix of eigenvectors\n",
        "                                        each column corresponds to a different\n",
        "                                        eigenvalue\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
        "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
        "  \"\"\"\n",
        "\n",
        "  index = np.flip(np.argsort(evals))\n",
        "  evals = evals[index]\n",
        "  evectors = evectors[:, index]\n",
        "  if evals.shape[0] == 2:\n",
        "    if np.arccos(np.matmul(evectors[:, 0],\n",
        "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 0] = -evectors[:, 0]\n",
        "    if np.arccos(np.matmul(evectors[:, 1],\n",
        "                           1 / np.sqrt(2) * np.array([-1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 1] = -evectors[:, 1]\n",
        "  return evals, evectors"
      ],
      "metadata": {
        "id": "MFHrynkgDU12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_of_basis(X, W):\n",
        "  \"\"\"\n",
        "  Projects data onto new basis W.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
        "                                basis vectors\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)    : Data matrix expressed in new basis\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: project the data onto a new basis W\n",
        "  # Fill out function and remove\n",
        "  #raise NotImplementedError(\"Student exercise: implement change of basis\")\n",
        "  #################################################\n",
        "\n",
        "  # Project data onto new basis described by W\n",
        "  Y = X @ W\n",
        "\n",
        "  return Y"
      ],
      "metadata": {
        "id": "SeL2AhY2DjyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting functions"
      ],
      "metadata": {
        "id": "pa4Rw6WADz9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_eigenvalues(evals):\n",
        "  \"\"\"\n",
        "  Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "      (numpy array of floats) : Vector of eigenvalues\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(4, 4))\n",
        "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
        "  plt.xlabel('Component')\n",
        "  plt.ylabel('Eigenvalue')\n",
        "  plt.title('Scree plot')\n",
        "  plt.xticks(np.arange(1, len(evals) + 1))\n",
        "  plt.ylim([0, 2.5])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_data(X):\n",
        "  \"\"\"\n",
        "  Plots bivariate data. Includes a plot of each random variable, and a scatter\n",
        "  scatter plot of their joint activity. The title indicates the sample\n",
        "  correlation calculated from the data.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax1 = fig.add_subplot(gs[0, 0])\n",
        "  ax1.plot(X[:, 0], color='k')\n",
        "  plt.ylabel('Neuron 1')\n",
        "  ax2 = fig.add_subplot(gs[1, 0])\n",
        "  ax2.plot(X[:, 1], color='k')\n",
        "  plt.xlabel('Sample Number (sorted)')\n",
        "  plt.ylabel('Neuron 2')\n",
        "  ax3 = fig.add_subplot(gs[:, 1])\n",
        "  ax3.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
        "           markeredgewidth=0)\n",
        "  ax3.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(X[:, 0], X[:, 1])[0, 1]))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_data_new_basis(Y):\n",
        "  \"\"\"\n",
        "  Plots bivariate data after transformation to new bases. Similar to plot_data\n",
        "  but with colors corresponding to projections onto basis 1 (red) and\n",
        "  basis 2 (blue).\n",
        "  The title indicates the sample correlation calculated from the data.\n",
        "\n",
        "  Note that samples are re-sorted in ascending order for the first random\n",
        "  variable.\n",
        "\n",
        "  Args:\n",
        "    Y (numpy array of floats) : Data matrix in new basis each column\n",
        "                                corresponds to a different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax1 = fig.add_subplot(gs[0, 0])\n",
        "  ax1.plot(Y[:, 0], 'r')\n",
        "  plt.ylabel('Projection \\n basis vector 1')\n",
        "  ax2 = fig.add_subplot(gs[1, 0])\n",
        "  ax2.plot(Y[:, 1], 'b')\n",
        "  plt.xlabel('Sample number')\n",
        "  plt.ylabel('Projection \\n basis vector 2')\n",
        "  ax3 = fig.add_subplot(gs[:, 1])\n",
        "  ax3.plot(Y[:, 0], Y[:, 1], '.', color=[.5, .5, .5])\n",
        "  ax3.axis('equal')\n",
        "  plt.xlabel('Projection basis vector 1')\n",
        "  plt.ylabel('Projection basis vector 2')\n",
        "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(Y[:, 0], Y[:, 1])[0, 1]))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_basis_vectors(X, W):\n",
        "  \"\"\"\n",
        "  Plots bivariate data as well as new basis vectors.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : Square matrix representing new orthonormal\n",
        "                                basis each column represents a basis vector\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure(figsize=[4, 4])\n",
        "  plt.plot(X[:, 0], X[:, 1], '.', color=[.5, .5, .5], label='Data')\n",
        "  plt.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  plt.plot([0, W[0, 0]], [0, W[1, 0]], color='r', linewidth=3,\n",
        "           label='Basis vector 1')\n",
        "  plt.plot([0, W[0, 1]], [0, W[1, 1]], color='b', linewidth=3,\n",
        "           label='Basis vector 2')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "uhcn0mGUDyex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "UDyQIR4bD_2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca(X):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats): Data matrix each column corresponds to a\n",
        "                               different random variable\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)  : Data projected onto the new basis\n",
        "    (numpy array of floats)  : Vector of eigenvalues\n",
        "    (numpy array of floats)  : Corresponding matrix of eigenvectors\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students: calculate the covariance matrix\n",
        "  # Fill out function and remove\n",
        "  #raise NotImplementedError(\"Student exercise: sort eigenvalues/eigenvectors!\")\n",
        "  #################################################\n",
        "\n",
        "  # Calculate the sample covariance matrix\n",
        "  cov_matrix = get_sample_cov_matrix(X)\n",
        "\n",
        "  # Calculate the eigenvalues and eigenvectors\n",
        "  evals, evectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "  # Sort the eigenvalues in descending order\n",
        "  evals, evectors = sort_evals_descending(evals, evectors)\n",
        "\n",
        "\n",
        "  # Project the data onto the new eigenvector basis\n",
        "  score = change_of_basis(X, evectors)\n",
        "\n",
        "  return score, evectors, evals\n",
        "\n",
        "\n",
        "# Perform PCA on the data matrix X\n",
        "#score, evectors, evals = pca(reshaped_matrix)\n",
        "\n",
        "# Plot the data projected into the new basis\n",
        "#plot_data_new_basis(score)"
      ],
      "metadata": {
        "id": "UEXo21OCY4KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data load"
      ],
      "metadata": {
        "id": "zr3EY85dBijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir directorios y archivos\n",
        "drive_dir = \"/content/drive/My Drive/HCP_data\"\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "fnames = [\"hcp_rest.tgz\", \"hcp_covariates.tgz\", \"atlas.npz\"]\n",
        "urls = [\"https://osf.io/bqp7m/download\",\n",
        "        \"https://osf.io/x5p4g/download\",\n",
        "        \"https://osf.io/j5kuc/download\"]\n",
        "\n",
        "# Función para descargar archivos\n",
        "def download_file(url, fname):\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()  # Check for HTTP errors\n",
        "    except requests.ConnectionError:\n",
        "        print(\"!!! Failed to download data: Connection Error !!!\")\n",
        "    except requests.HTTPError as err:\n",
        "        print(f\"!!! Failed to download data: HTTP Error {err} !!!\")\n",
        "    else:\n",
        "        print(f\"Downloading {fname}...\")\n",
        "        with open(fname, \"wb\") as fid:\n",
        "            fid.write(r.content)\n",
        "        print(f\"Download {fname} completed!\")\n",
        "\n",
        "# Descargar los archivos si no existen en Google Drive\n",
        "for fname, url in zip(fnames, urls):\n",
        "    drive_path = os.path.join(drive_dir, fname)\n",
        "    if not os.path.isfile(drive_path):\n",
        "        download_file(url, drive_path)\n",
        "    else:\n",
        "        print(f\"{fname} already exists in Google Drive.\")\n",
        "\n",
        "# Si necesitas extraer los archivos\n",
        "def extract_file(filepath, extract_to):\n",
        "    if tarfile.is_tarfile(filepath):\n",
        "        with tarfile.open(filepath) as tar:\n",
        "            tar.extractall(path=extract_to)\n",
        "        print(f\"Extracted {os.path.basename(filepath)} to {extract_to}\")\n",
        "    else:\n",
        "        print(f\"{os.path.basename(filepath)} is not a valid tar file.\")\n",
        "\n",
        "for fname in fnames:\n",
        "    drive_path = os.path.join(drive_dir, fname)\n",
        "    extract_file(drive_path, drive_dir)\n"
      ],
      "metadata": {
        "id": "MEcJVvtrWjF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe28207-0a4d-47aa-be2e-9777ac685333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "hcp_rest.tgz already exists in Google Drive.\n",
            "hcp_covariates.tgz already exists in Google Drive.\n",
            "atlas.npz already exists in Google Drive.\n",
            "Extracted hcp_rest.tgz to /content/drive/My Drive/HCP_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8x__qEVWKEx"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn --quiet\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Necessary for visualization\n",
        "from nilearn import plotting, datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"/content/drive/My Drive/HCP_data\"\n",
        "if not os.path.isdir(HCP_DIR):\n",
        "  os.mkdir(HCP_DIR)\n",
        "\n",
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 339\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in sec\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated multiple times in each subject\n",
        "N_RUNS_REST = 4\n",
        "N_RUNS_TASK = 2\n",
        "\n",
        "# Time series data are organized by experiment, with each experiment\n",
        "# having an LR and RL (phase-encode direction) acquistion\n",
        "BOLD_NAMES = [\n",
        "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
        "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
        "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
        "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
        "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
        "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
        "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
        "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
        "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
        "]\n",
        "\n",
        "# You may want to limit the subjects used during code development.\n",
        "# This will use all subjects:\n",
        "subjects = range(N_SUBJECTS)"
      ],
      "metadata": {
        "id": "E4Fa8uToWbzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "language_csv = '/content/drive/My Drive/HCP_data/hcp/behavior/language.csv'\n",
        "language_df = pd.read_csv(language_csv)\n",
        "filtered_df = language_df[language_df['ConditionName'] == 'STORY']\n",
        "filtered_df\n",
        "run_mean = filtered_df.groupby('Subject')[['ACC', 'AVG_DIFFICULTY_LEVEL', 'MEDIAN_RT']].mean().reset_index()\n",
        "\n",
        "avg_dif_df = run_mean[['Subject', 'AVG_DIFFICULTY_LEVEL']]\n",
        "\n",
        "avg_dif_df.head()\n",
        "\n",
        "#plt.hist(run_mean['ACC'])\n",
        "plt.hist(run_mean['AVG_DIFFICULTY_LEVEL'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwyG-oK25w3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el umbral\n",
        "threshold = 10.5\n",
        "\n",
        "# Filtrar los datos por debajo del umbral\n",
        "below_threshold_df = run_mean[run_mean['AVG_DIFFICULTY_LEVEL'] < threshold]\n",
        "\n",
        "# Filtrar los datos por encima o iguales al umbral\n",
        "above_threshold_df = run_mean[run_mean['AVG_DIFFICULTY_LEVEL'] >= threshold]\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Below threshold:\")\n",
        "print(below_threshold_df)\n",
        "\n",
        "print(\"\\n above the threshold:\")\n",
        "print(above_threshold_df)\n"
      ],
      "metadata": {
        "id": "hA88vwJfTdKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una nueva columna 'LABEL' basada en la condición del umbral\n",
        "run_mean['LABEL'] = (run_mean['AVG_DIFFICULTY_LEVEL'] >= threshold).astype(int)\n",
        "\n",
        "# Filtrar los datos por debajo del umbral\n",
        "below_threshold_df = run_mean[run_mean['LABEL'] == 0]\n",
        "\n",
        "# Filtrar los datos por encima o iguales al umbral\n",
        "above_threshold_df = run_mean[run_mean['LABEL'] == 1]\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Below threshold:\")\n",
        "print(below_threshold_df)\n",
        "\n",
        "print(\"\\nAbove the threshold:\")\n",
        "print(above_threshold_df)"
      ],
      "metadata": {
        "id": "2NlFbgK8BPEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MATRIX LOADING"
      ],
      "metadata": {
        "id": "bHXSfR8MCvC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_ids(name):\n",
        "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
        "\n",
        "    Args:\n",
        "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    Returns:\n",
        "      run_ids (list of int) : Numeric ID for experiment image files\n",
        "\n",
        "  \"\"\"\n",
        "  run_ids = [\n",
        "             i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
        "             ]\n",
        "  if not run_ids:\n",
        "    raise ValueError(f\"Found no data for '{name}'\")\n",
        "  return run_ids\n",
        "\n",
        "\n",
        "def load_timeseries(subject, name, dir,\n",
        "                    runs=None, concat=True, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    dir (str) : data directory\n",
        "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
        "      or None to load all runs.\n",
        "    concat (bool) : If True, concatenate multiple runs in time\n",
        "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  # Get the list relative 0-based index of runs to use\n",
        "  if runs is None:\n",
        "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
        "  elif isinstance(runs, int):\n",
        "    runs = [runs]\n",
        "\n",
        "  # Get the first (1-based) run id for this experiment\n",
        "  offset = get_image_ids(name)[0]\n",
        "\n",
        "  # Load each run's data\n",
        "  bold_data = [\n",
        "               load_single_timeseries(subject,\n",
        "                                      offset + run,\n",
        "                                      dir,\n",
        "                                      remove_mean) for run in runs\n",
        "               ]\n",
        "\n",
        "  # Optionally concatenate in time\n",
        "  if concat:\n",
        "    bold_data = np.concatenate(bold_data, axis=-1)\n",
        "\n",
        "  return bold_data\n",
        "\n",
        "\n",
        "def load_single_timeseries(subject, bold_run, dir, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    bold_run (int): 1-based run index, across all tasks\n",
        "    dir (str) : data directory\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_path = os.path.join(dir, \"subjects\", str(subject), \"timeseries\")\n",
        "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
        "  ts = np.load(os.path.join(bold_path, bold_file))\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "\n",
        "def load_evs(subject, name, condition, dir):\n",
        "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of task\n",
        "    condition (str) : Name of condition\n",
        "    dir (str) : data directory\n",
        "\n",
        "  Returns\n",
        "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
        "      of the condition for each run.\n",
        "\n",
        "  \"\"\"\n",
        "  evs = []\n",
        "  for id in get_image_ids(name):\n",
        "    task_key = BOLD_NAMES[id - 1]\n",
        "    ev_file = os.path.join(dir, \"subjects\", str(subject), \"EVs\",\n",
        "                           task_key, f\"{condition}.txt\")\n",
        "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "    evs.append(ev)\n",
        "  return evs"
      ],
      "metadata": {
        "id": "Db2gqlxDWwB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract and concatenate timeseries"
      ],
      "metadata": {
        "id": "TpOCrZ8_-5zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries = load_timeseries(subject=0,\n",
        "                             name=\"rest\",\n",
        "                             dir=os.path.join(HCP_DIR, \"hcp_rest\"),\n",
        "                             runs=1)\n",
        "print(timeseries.shape)  # n_parcel x n_timepoint"
      ],
      "metadata": {
        "id": "BkbbyxC3W2Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries_rest = []\n",
        "for subject in subjects:\n",
        "  ts_concat = load_timeseries(subject, name=\"rest\",\n",
        "                              dir=os.path.join(HCP_DIR, \"hcp_rest\"))\n",
        "  timeseries_rest.append(ts_concat)"
      ],
      "metadata": {
        "id": "u3nB8yg6W5bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct functional connectivity matrix (FC)"
      ],
      "metadata": {
        "id": "mrgV1NSC--Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_connectivity_matrix = np.zeros((N_SUBJECTS, N_PARCELS, N_PARCELS))\n",
        "for sub, ts in enumerate(timeseries_rest):\n",
        "  f_connectivity_matrix[sub] = np.corrcoef(ts)\n",
        "\n",
        "group_fc = f_connectivity_matrix.mean(axis=0)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(group_fc, interpolation=\"none\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T7u4lBi2W_mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_triangle = np.tril(f_connectivity_matrix, k=-1)"
      ],
      "metadata": {
        "id": "OVfQhFll_pvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_fc_lower = lower_triangle.mean(axis=0)\n",
        "plt.figure()\n",
        "plt.imshow(lower_triangle[1,:,:], interpolation=\"none\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vL4DpgRiAqUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA and tSNE\n",
        "\n"
      ],
      "metadata": {
        "id": "WzaejrU0G9vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Screeplot"
      ],
      "metadata": {
        "id": "3GO54gfg17yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TA advise: define a function that thresholds the accuracy. make a vector of n_subjects and assing the label 0 or 1 based on a threshold, (we can have more groups but less group the model will perform better)"
      ],
      "metadata": {
        "id": "FcmIX8f6MW_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para asignar etiquetas basadas en un umbral\n",
        "def assign_labels(dataframe, column, threshold):\n",
        "    labels = (dataframe[column] > threshold).astype(int)\n",
        "    return labels\n",
        "\n",
        "# Asignar etiquetas\n",
        "avg_dif_df['Label'] = assign_labels(avg_dif_df, 'AVG_DIFFICULTY_LEVEL', threshold)\n",
        "\n",
        "# Mostrar el DataFrame con las etiquetas\n",
        "print(avg_dif_df)\n",
        "\n",
        "# Crear un histograma de 'AVG_DIFFICULTY_LEVEL' con etiquetas\n",
        "plt.hist(avg_dif_df['AVG_DIFFICULTY_LEVEL'], bins=30, alpha=0.5, label='AVG_DIFFICULTY_LEVEL')\n",
        "plt.axvline(threshold, color='r', linestyle='dashed', linewidth=1, label=f'Threshold = {threshold:.2f}')\n",
        "plt.xlabel('AVG_DIFFICULTY_LEVEL')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of AVG_DIFFICULTY_LEVEL with Threshold')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Vector de etiquetas\n",
        "labels_vector = avg_dif_df['Label'].values\n",
        "print(labels_vector)"
      ],
      "metadata": {
        "id": "f2tnVksYAZid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que ya has calculado la matriz de conectividad grupal\n",
        "# group_fc tiene dimensiones (N_PARCELS, N_PARCELS)\n",
        "\n",
        "# Aplanar la matriz de conectividad funcional para cada sujeto\n",
        "lower_triangle_indices = np.tril_indices(N_PARCELS, k=-1)\n",
        "flat_fc = [fc[lower_triangle_indices] for fc in f_connectivity_matrix]\n",
        "\n",
        "# Convertir a un array de numpy\n",
        "flat_fc = np.array(flat_fc)\n",
        "\n",
        "print(flat_fc.shape)  # Debería ser (N_SUBJECTS, N_PARCELS*(N_PARCELS-1)/2)\n"
      ],
      "metadata": {
        "id": "NBy3OpmbLOBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(flat_fc)\n",
        "\n",
        "# Step 3: Perform PCA\n",
        "pca = PCA(n_components=50)  # You can adjust the number of components\n",
        "principal_components = pca.fit_transform(standardized_data)\n",
        "\n",
        "# Step 4: Visualize the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_components[:, 0], principal_components[:, 1], c='blue')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Subjects using Lower Triangle')\n",
        "plt.show()\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "id": "dpoVNGkoFBoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1fkhpqPZxy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Explained variance\n",
        "explained_variance = pca.explained_variance_\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# # # Plot eigenvalues (explained variance)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Eigenvalue (Explained Variance)')\n",
        "plt.title('Eigenvalues of Principal Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# # # Plot cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(explained_variance_ratio), marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g1jQm4Xx162h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of components needed to explain 90% of the variance\n",
        "explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components_90 = np.argmax(explained_variance_ratio >= 0.90) + 1\n",
        "n_components_75 = np.argmax(explained_variance_ratio >= 0.75) + 1\n",
        "n_components_50 = np.argmax(explained_variance_ratio >= 0.50) + 1\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Number of components to retain 90% variance: {n_components_90}\")\n",
        "print(f\"Number of components to retain 75% variance: {n_components_75}\")\n",
        "print(f\"Number of components to retain 50% variance: {n_components_50}\")"
      ],
      "metadata": {
        "id": "vTGTnQ_Y3GeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can try to do a linear regression to predict the avg_difficult instead of a clasiffier and we won't need the labels"
      ],
      "metadata": {
        "id": "i6wFXpUPNLVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supongamos que tienes datos escalados X_scaled\n",
        "\n",
        "# Aplicar PCA sin especificar el número de componentes\n",
        "pca = PCA()\n",
        "pca.fit(standardized_data)\n",
        "\n",
        "# Obtener la varianza explicada acumulada\n",
        "explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Graficar la varianza explicada acumulada\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('Varianza Explicada Acumulada')\n",
        "plt.title('Varianza Explicada Acumulada por Componentes Principales')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.90, color='r', linestyle='--')  # Línea para 90% de varianza explicada\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S2MNPCC32JI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The model starts here"
      ],
      "metadata": {
        "id": "G3YTUjNIEesI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_dif_df"
      ],
      "metadata": {
        "id": "eHzN4rPAEgr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_vector = run_mean['LABEL'].values\n",
        "print(labels_vector)\n",
        "print(labels_vector.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "FT8paIcJEgpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principal_components"
      ],
      "metadata": {
        "id": "lu5B7VvkFoU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try other models\n",
        "random forest, decision making trees, svm,\n",
        "feature selection (other than pca)\n",
        "knearest neighbour\n"
      ],
      "metadata": {
        "id": "z_v4B3s5XXbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Assuming principal_components is your feature data and y is your target variable\n",
        "X = principal_components\n",
        "y = labels_vector  # Replace with your actual target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42)\n",
        "\n",
        "# Initialize the classifier\n",
        "classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)\n",
        "\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(matrix)\n"
      ],
      "metadata": {
        "id": "JSWC8evzHdBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(principal_components.shape)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "U92nCWADD144"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "qE8mqOMjIHAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute the confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classifier.classes_, yticklabels=classifier.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hUJU3BmpHc0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Validation"
      ],
      "metadata": {
        "id": "BJeFBqr8L4S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k fold validation,"
      ],
      "metadata": {
        "id": "8yONoxU_YXr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(classifier, X, y, cv=5)\n",
        "\n",
        "print(f'Cross-validation scores: {cv_scores}')\n",
        "print(f'Mean CV score: {cv_scores.mean()}')\n"
      ],
      "metadata": {
        "id": "2y_tmzm7HcwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance"
      ],
      "metadata": {
        "id": "SahDMnIFL_D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Feature importance for logistic regression\n",
        "if hasattr(classifier, 'coef_'):\n",
        "    importance = np.abs(classifier.coef_[0])\n",
        "    feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
        "    feature_importance = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print('Feature Importance:')\n",
        "    for feature, importance in feature_importance:\n",
        "        print(f'{feature}: {importance}')\n",
        "\n",
        "    # Plot feature importance\n",
        "    feature_names, importances = zip(*feature_importance)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(feature_names, importances, align='center')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.gca().invert_yaxis()  # Invert y axis to have the most important feature at the top\n",
        "    plt.show()\n",
        "else:\n",
        "    print('The classifier does not have a `coef_` attribute.')"
      ],
      "metadata": {
        "id": "KRZ39yuaHcuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "peXC1k9zHVDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGboost\n"
      ],
      "metadata": {
        "id": "IgaEx78qPBEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize the classifier\n",
        "classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute the confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classifier.classes_, yticklabels=classifier.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o36AN5VDJV2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search XGboost"
      ],
      "metadata": {
        "id": "l832iq2DQJEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [1, 1.5, 2]\n",
        "}\n",
        "\n",
        "# Initialize the classifier\n",
        "classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_}')\n",
        "\n",
        "# Use the best estimator to make predictions\n",
        "best_classifier = grid_search.best_estimator_\n",
        "y_pred = best_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifiergit\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "1-opter6JVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# elbow kmeans"
      ],
      "metadata": {
        "id": "WPquR_ikDlgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Using t-SNE results with perplexity=30 for clustering\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "tsne_result = tsne.fit_transform(features)\n",
        "\n",
        "# Elbow Method\n",
        "inertia = []\n",
        "k_values = range(1, 11)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(tsne_result)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_values, inertia, 'bo-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jOIeQ8fjDlH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest"
      ],
      "metadata": {
        "id": "c1upvNGw9hWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have X_train, y_train, X_test, and y_test defined\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "aGsVg57B9gZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Silhouette Score"
      ],
      "metadata": {
        "id": "wO5IoepHDszf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Silhouette Score Method\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(tsne_result)\n",
        "    silhouette_scores.append(silhouette_score(tsne_result, labels))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(2, 11), silhouette_scores, 'bo-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Optimal k')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G1QAW5JTDcJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Using t-SNE results with perplexity=30 for clustering\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "tsne_result = tsne.fit_transform(features)\n",
        "\n",
        "# Perform K-means clustering with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(tsne_result)\n",
        "\n",
        "# Add cluster labels to the run_mean DataFrame\n",
        "run_mean['Cluster'] = labels\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
        "plt.title('t-SNE with K-means Clustering (k=3)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(scatter, orientation='horizontal', label='Cluster Label')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Analyze the clusters\n",
        "cluster_summary = run_mean.groupby('Cluster')[['ACC', 'AVG_DIFFICULTY_LEVEL', 'MEDIAN_RT']].mean()\n",
        "print(cluster_summary)\n",
        "\n",
        "print('Cluster 0 (Cyan), Cluster 1 (Yellow), Cluster 2 (Purple)')"
      ],
      "metadata": {
        "id": "b9a5gAHnEg03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe the data to check for anomalies\n",
        "print(run_mean.describe())\n",
        "\n",
        "# Plot the features to check for anomalies\n",
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(run_mean, hue='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M2kylmQXGsMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for correlations\n",
        "correlation_matrix = run_mean.corr()\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "# Drop one of each pair of highly correlated features if necessary\n",
        "# For example, if ACC and AVG_DIFFICULTY_LEVEL are highly correlated, you might drop one.\n"
      ],
      "metadata": {
        "id": "1hcKXgRVIQtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for class imbalance\n",
        "print(run_mean['Cluster'].value_counts())\n",
        "\n",
        "# Visualize the class distribution\n",
        "sns.countplot(x='Cluster', data=run_mean)\n",
        "plt.title('Class Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gpNE_bnKJG6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single person connectivity matrix"
      ],
      "metadata": {
        "id": "n7yTMD-t_4-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder"
      ],
      "metadata": {
        "id": "sW_LWJi2dmY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "bYrL1TzegEK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Assuming N_SUBJECTS and N_PARCELS are defined elsewhere\n",
        "\n",
        "# Reshape fc array (choose your preferred method)\n",
        "fc_reshaped = fc.reshape(N_SUBJECTS, -1)  # Flatten the entire connectivity matrix\n",
        "\n",
        "# Define Autoencoder Model\n",
        "class Autoencoder(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(input_shape=(fc_reshaped.shape[1],)),  # Adjust for your reshaping method\n",
        "      layers.Dense(128, activation=\"relu\"),\n",
        "      layers.Dense(64, activation=\"relu\"),\n",
        "      layers.Dense(latent_dim, activation=\"relu\"),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(64, activation=\"relu\"),\n",
        "      layers.Dense(128, activation=\"relu\"),\n",
        "      layers.Dense(fc_reshaped.shape[1]),  # Adjust for your reshaping method\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "# Define Hyperparameters (adjust as needed)\n",
        "latent_dim = 32  # Dimensionality of the latent space\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Create the Autoencoder model\n",
        "model = Autoencoder(latent_dim)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(fc_reshaped, fc_reshaped, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Extract Latent Features (optional)\n",
        "latent_features = model.encoder.predict(fc_reshaped)\n"
      ],
      "metadata": {
        "id": "tN3pfgBIdl11",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}